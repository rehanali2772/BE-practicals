# -*- coding: utf-8 -*-
"""Bank Customer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xV-r_cFuQwWplYptI8R-lpE5pdAZbVF
"""

# -----------------------------------------------------------
# 1. Import Libraries
# -----------------------------------------------------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import plot_model

# -----------------------------------------------------------
# 2. Load Dataset
# -----------------------------------------------------------

df = pd.read_csv("Churn_Modelling.csv")   # upload the file to colab

print("\nDataset Loaded Successfully\n")
df.head()

# -----------------------------------------------------------
# 3. Feature & Target Separation
# -----------------------------------------------------------

# Features from columns 3â€“12
X = df.iloc[:, 3:13].values

# Target variable
y = df.iloc[:, 13].values

print("Feature Matrix Shape:", X.shape)
print("Target Vector Shape:", y.shape)

# -----------------------------------------------------------
# 4. Encoding Categorical Variables
# -----------------------------------------------------------

# Label encoding for Gender
le = LabelEncoder()
X[:, 2] = le.fit_transform(X[:, 2])

# One-Hot Encoding for Geography column
geo_encoder = OneHotEncoder(drop="first", sparse_output=False)
geo = geo_encoder.fit_transform(X[:, 1].reshape(-1, 1))

# Replace old Geography column with new encoded columns
X = np.delete(X, 1, axis=1)
X = np.concatenate((geo, X), axis=1)

print("Shape after Encoding:", X.shape)

# -----------------------------------------------------------
# 5. Split into Training & Testing
# -----------------------------------------------------------

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Training Set Shape:", X_train.shape)
print("Testing Set Shape:", X_test.shape)

# -----------------------------------------------------------
# 6. Normalize the Data
# -----------------------------------------------------------

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("Data Normalized Successfully")

# -----------------------------------------------------------
# 7. Build the Neural Network Model
#    (With Improvements)
# -----------------------------------------------------------

model = Sequential()

# Input Layer + Hidden Layer 1
model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.2))     # improvement: dropout for regularization

# Hidden Layer 2
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.2))     # improvement: prevent overfitting

# Output layer
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# -----------------------------------------------------------
# 8. Train the Model
# -----------------------------------------------------------

history = model.fit(
    X_train, y_train,
    batch_size=32,
    epochs=50,
    validation_split=0.2,
    verbose=1
)

# -----------------------------------------------------------
# 9. Predictions & Evaluation
# -----------------------------------------------------------

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Accuracy
acc = accuracy_score(y_test, y_pred)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

print("\n==== MODEL PERFORMANCE ====")
print("Accuracy Score:", acc)
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))